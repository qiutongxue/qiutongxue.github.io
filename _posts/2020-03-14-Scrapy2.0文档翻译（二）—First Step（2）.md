---
layout: post
author: Berry Qiu
title: Scrapy2.0文档翻译（二）—First Step（2）
subtitle: 这是 First Step 的第二部分
cover: 
color: '' # 默认根据字数生成相应颜色
tags: ['Python', '文档']
category: public
---

## Scrapy 教程

在本教程中，我们假设您的系统中已经安装好了 Scrapy，如果没有，请看[安装指南]()

我们将爬取的是列有名人名言的网站： [quotes.toscrape.com](http://quotes.toscrape.com/)

本教程将指导您完成以下任务：

1. 创建一个新的 Scrapy 项目
2. 写一个 [spider]() 来爬取网站及提取数据
3. 通过命令行导出爬取到的数据
4. 改变 spider 以递归跟踪链接
5. 使用 spider 参数

Scrapy用Python编写。 如果您是该语言的新手，则可能首先要了解该语言，以充分利用Scrapy。

如果您已经熟悉其他语言，并且想快速学习Python，那么[这个Python教程](https://docs.python.org/3/tutorial)是一个很好的资源。

### 创建项目

在开始爬虫之前，你得创建一个新的 Scrapy 项目。进入一个你想存放代码的目录，并运行：

```
scrapy startproject tutorial
```

这将创建一个包含以下内容的 `tutorial` 目录：

```bash
tutorial/
    scrapy.cfg            # 部署配置文件

    tutorial/             # 项目的Python模块，您将从此处导入代码
        __init__.py

        items.py          # 项目 item 定义文件

        middlewares.py    # 项目中间件文件

        pipelines.py      # 项目管道 pipeline 文件

        settings.py       # 项目设置文件

        spiders/          # 存放爬虫 spider 的目录
            __init__.py
```

### 第一个爬虫 （Spider）

爬虫是您定义的类，Scrapy 用其从网站（或一组网站）中获取信息。他们必须是 `Spider` 的子类，并定义要发出的初始请求，可以选择如何跟随页面中的链接，以及如何解析下载的页面内容以提取数据。

以下是我们的第一个爬虫的代码。将其存储在您项目目录的 `tutorial/spiders` 下并命名为 `quotes_spider.py` 。

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
```

正如你所见，我们的爬虫继承自 `scrapy.Spider` ，并定义了一些属性和方法：

- `name`：爬虫的标识名。 它在一个项目中必须是唯一的，也就是说，您不能为不同的Spider设置相同的名称。

- `start_requests()`：必须返回一个可迭代的请求（您可以返回一个请求列表或写一个生成器函数），Spider 将从中开始爬取。 随后的请求将根据这些初始请求连续生成。

- `parse()`：一个被调用以处理针对每个 request 下载的响应的方法。 response参数是`TextResponse` 的一个实例，该实例保存页面内容并具有其他有用的方法来处理它。

  `parse()` 方法通常解析响应，提取爬到的数据作为字典，还查找新URL并从中创建新请求（`Request`）。

#### 怎么让爬虫动起来？

为了让我们的爬虫能够工作，请转到该项目的顶级目录并执行：

```
scrapy crawl quotes
```

该命令使我们刚刚添加并命名为 `quotes` 的爬虫开始工作，该爬虫会向 `quotes.toscrape.com` 域名发送请求。你将会看到像下面这样的输出信息：

```
... (omitted for brevity)
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened
2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)
...
```

现在检查看看当前目录下的文件。你应该注意到生成了两个新的文件： `quotes-1.html` 和 `quotes-2.html`，其中包含我们URL的内容，正如 `parse` 方法里写的那样。

注意：如果您想知道为什么我们还没有解析HTML，请稍等，我们很快就会讲到。

#####  刚刚发生了什么？

Scrapy 调度 Spider 的 `start_requests` 方法返回的 `scrapy.Request` 对象。 在收到每个请求的响应时，它实例化 `Response` 对象并调用**其请求Request中的回调方法**（在本例中为 `parse` 方法），将响应作为参数传递。

#### start_requests 方法的快捷方式

无需实现从URL生成 `scrapy.Request` 对象的 `start_requests()`方法，您只需定义带有URL列表的 `start_urls` 类属性即可。 然后，`start_requests()` 的默认实现将使用此列表来为您的爬虫创建初始请求：

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
```

即使我们没有明确告诉 Scrapy 这样做，也会调用 `parse() `方法来处理对这些URL的每个请求。 发生这种情况是因为 `parse()` 是 Scrapy 的默认回调方法，对于没有显式分配的回调的请求会调用该方法。

#### 提取数据

学习如何使用Scrapy提取数据的最佳方法是使用 [Scrapy shell]() 去尝试选择器。 运行：

```
scrapy shell 'http://quotes.toscrape.com/page/1/'
```

> 请记住，从命令行运行Scrapy shell时，始终将网址括在引号中，否则包含参数（即＆字符）的网址将不起作用。
>
> 在 Windows 中，用双引号：
>
> ```powershell
> scrapy shell "http://quotes.toscrape.com/page/1/"
> ```

你将会看到类似下面的信息：

```
[ ... Scrapy log here ... ]
2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x7fa91d888c90>
[s]   item       {}
[s]   request    <GET http://quotes.toscrape.com/page/1/>
[s]   response   <200 http://quotes.toscrape.com/page/1/>
[s]   settings   <scrapy.settings.Settings object at 0x7fa91d888c10>
[s]   spider     <DefaultSpider 'default' at 0x7fa91c8af990>
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser
```

使用该 shell，您可以尝试使用带有响应对象的CSS选择元素：

```python
>>> response.css('title')
[<Selector xpath='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]
```

运行 `response.css('title')` 的结果是一个名为 `SelectorList` 的类似列表的对象，该对象表示包含 XML / HTML 元素的 `Selector` 对象的列表，并允许您运行进一步的查询来细化选择或提取内容数据。

要提取标题的文本，你可以：

```python
>>> response.css('title::text').getall()
['Quotes to Scrape']
```

这里有两点需要注意：一是我们在CSS查询中添加了 `::text`，这意味着我们只想直接在 `<title>`元素内选择 text 元素。 如果不指定 `::text`，则会获得完整的 title 元素，包括其标签：

```python
>>> response.css('title').getall()
['<title>Quotes to Scrape</title>']
```

另一点是，调用 `.getall()` 的结果是一个列表：选择器有可能返回多个结果，因此我们将它们全部提取出来。 当您知道只想要第一个结果时，在这种情况下，您可以执行以下操作：

```python
>>> response.css('title::text').get()
'Quotes to Scrape'
```

或者您可以这样：

```python
>>> response.css('title::text')[0].get()
'Quotes to Scrape'
```

但是，直接在 `SelectorList` 实例上使用 `.get()` 可以避免 `IndexError` ，并且在找不到与选择匹配的任何元素时返回 `None`。

这里有一个教训：对于大多数抓取代码，您希望它能够对**由于页面上找不到内容而导致的错误**具有弹性，以便即使某些部分未能被抓取，您也至少可以获取一些数据。

除了 `getall()` 和 `get()` 方法，你也可以通过 `re()` 方法用[正则表达式](https://docs.python.org/3/library/re.html)提取：

```python
>>> response.css('title::text').re(r'Quotes.*')
['Quotes to Scrape']
>>> response.css('title::text').re(r'Q\w+')
['Quotes']
>>> response.css('title::text').re(r'(\w+) to (\w+)')
['Quotes', 'Scrape']
```

为了能找到合适的 CSS 选择器来使用，在 shell 中使用 `view(response)` 来在web浏览器中打开相应页面或许是一个很有用的方法。您可以使用浏览器中的开发者工具来检查 HTML 并找到合适的选择器。

[Selector Gadget](https://selectorgadget.com/)是一个不错的工具，可以快速为视觉选择的元素找到CSS选择器，可在许多浏览器中使用。

##### 简单地介绍一下 XPath

除了 CSS，Scrapy 选择器也支持使用 XPath 表达式：

```python
>>> response.xpath('//title')
[<Selector xpath='//title' data='<title>Quotes to Scrape</title>'>]
>>> response.xpath('//title/text()').get()
'Quotes to Scrape'
```

XPath表达式非常强大，并且是Scrapy Selectors的基础。 实际上，CSS选择器是在后台转换为XPath的。如果您仔细阅读 shell 中选择器对象的文本表示形式，您也能发现这一点。

尽管XPath表达式可能不如CSS选择器流行，但它提供了更多功能，因为除了导航结构之外，它还可以查看内容。 使用XPath，您可以选择以下内容：选择包含文本 “*下一页*” 的链接。 这使XPath非常适合于抓取任务，并且即使您已经知道如何构造CSS选择器，我们也鼓励您学习XPath，这将使抓取更加容易。

我们不会在这里过多地介绍XPath，但是您可以[在此处阅读有关将XPath与Scrapy Selectors结合使用]()的更多信息。 要了解有关XPath的更多信息，我们建议[这个教程——通过示例学习XPath](http://plasmasturm.org/log/xpath101/)，并建议[这个教程——学习“如何以XPath的思想思考”](http://plasmasturm.org/log/xpath101/)。

#####  提取名人名言

现在您对选择和提取有所了解，让我们通过编写代码从网页中提取名言来完善这个爬虫。

[http://quotes.toscrape.com](http://quotes.toscrape.com/) 中的每一句名言都通过 HTML 表示，向下面这样：

```html
<div class="quote">
    <span class="text">“The world as we have created it is a process of our
    thinking. It cannot be changed without changing our thinking.”</span>
    <span>
        by <small class="author">Albert Einstein</small>
        <a href="/author/Albert-Einstein">(about)</a>
    </span>
    <div class="tags">
        Tags:
        <a class="tag" href="/tag/change/page/1/">change</a>
        <a class="tag" href="/tag/deep-thoughts/page/1/">deep-thoughts</a>
        <a class="tag" href="/tag/thinking/page/1/">thinking</a>
        <a class="tag" href="/tag/world/page/1/">world</a>
    </div>
</div>
```

让我们打开 scrapy shell 来看看怎样提取我们想要的信息：

```powershell
scrapy shell 'http://quotes.toscrape.com'
```

我们得到了一个与 quote 相关的 HTML 元素的选择器列表：

```python
>>> response.css("div.quote")
[<Selector xpath="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>,
 <Selector xpath="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>,
 ...]
```

上面的查询返回的每个选择器都允许我们在其子元素上运行进一步的查询。 让我们将第一个选择器分配给变量，以便我们可以直接在特定的名言上运行CSS选择器：

```python
>>> quote = response.css("div.quote")[0]
```

现在，让我们使用刚刚创建的 `quote` 对象从名人名言中提取 `text`，`author` 和 `tags`：

```python
>>> text = quote.css("span.text::text").get()
>>> text
'“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'
>>> author = quote.css("small.author::text").get()
>>> author
'Albert Einstein'
```

鉴于 tags 是一个字符串列表，我们可以用 `getall()` 方法获得其全部信息：

```python
>>> tags = quote.css("div.tags a.tag::text").getall()
>>> tags
['change', 'deep-thoughts', 'thinking', 'world']
```

在弄清楚如何提取每一位之后，我们现在可以遍历所有名人名言元素并将它们放到Python字典中：

```python
>>> for quote in response.css("div.quote"):
...     text = quote.css("span.text::text").get()
...     author = quote.css("small.author::text").get()
...     tags = quote.css("div.tags a.tag::text").getall()
...     print(dict(text=text, author=author, tags=tags))
{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}
{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}
...
```

#### 在我们的爬虫中提取数据

让我们回到我们的爬虫中。到目前为止，它没有特别提取任何数据，只是将整个HTML页面保存到本地文件中。 让我们将上面的提取逻辑集成到我们的Spider中。

Scrapy爬虫通常会生成许多字典，其中包含从页面中提取的数据。 为此，我们在回调中使用`yield ` 关键字，如下所示：

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }
```

如果运行此爬虫，将在日志中输出提取到的数据：

```
2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
{'tags': ['life', 'love'], 'author': 'André Gide', 'text': '“It is better to be hated for what you are than to be loved for what you are not.”'}
2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
{'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': "“I have not failed. I've just found 10,000 ways that won't work.”"}
```

### 储存爬取到的数据

存储数据的最简单的方式是按照以下命令导出文件 （feed exports）：

```
scrapy crawl quotes -o quotes.json
```

这会生成一个 `quotes.json` 文件，该文件包含以 [JSON](https://en.wikipedia.org/wiki/JSON) 序列化的所有抓取项。

由于历史原因，Scrapy 会附加到给定文件，而不是覆盖其内容。 如果您两次运行此命令而没有在第二次之前删除该文件，那么最终将得到一个损坏的JSON文件。

你也可以使用其它格式，比如 [JSON Lines](http://jsonlines.org/)：

```
scrapy crawl quotes -o quotes.jl
```

JSON Lines格式很有用，因为它像流一样，您可以轻松地向其添加新记录。 当您运行两次时，就不会遇到JSON的相同问题。 另外，由于每条记录都是单独的一行，因此您可以处理大文件而不必将所有内容都放入内存中，因此有工具可以在命令行中帮助完成此任务，比如 [JQ](https://stedolan.github.io/jq)。

在小型项目中（例如本教程中的项目），这应该足够了。 但是，如果要对这些抓取项执行更复杂的操作，则可以编写一个[项目管道（Item Pipeline）]()。 创建项目时，已在`tutorial/pipelines.py`中为您预置了“项目管道”的文件。不过，如果只想存储抓取的项，则不需要实现任何项目管道。

### 跟进链接

假设您不仅需要从 [http://quotes.toscrape.com](http://quotes.toscrape.com) 的前两个页面中抓取内容，还希望从网站的所有页面中爬取名人名言。

现在您知道了如何从页面提取数据，让我们看看如何跟踪页面中的链接。

首先是提取我们要跟进的页面的链接。 检查我们的页面，我们可以看到带有以下标记的指向下一页的链接：

```html
<ul class="pager">
    <li class="next">
        <a href="/page/2/">Next <span aria-hidden="true">&rarr;</span></a>
    </li>
</ul>
```

我们可以在 shell 中尝试提取它：

```python
>>> response.css('li.next a').get()
'<a href="/page/2/">Next <span aria-hidden="true">→</span></a>'
```

这获得了anchor 元素，但是我们需要属性 `href`。 为此，Scrapy 支持可让您选择属性内容的CSS扩展，如下所示：

```python
>>> response.css('li.next a::attr(href)').get()
'/page/2/'
```

也可以使用 `attrib` 属性（相关更多信息，请参见[选择元素属性]()）：

```python
>>> response.css('li.next a').attrib['href']
'/page/2/'
```

现在让我们看一下我们的Spider，将其修改为以递归方式链接到下一页的链接，并从中提取数据：

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)
```

现在，在提取数据之后，`parse()` 方法将查找到下一页的链接，使用 `urljoin()` 方法构建完整的绝对URL（因为链接可以是相对的），并产生对下一页的新请求， 将其自身注册为回调函数，以处理下一页的数据提取并能爬取到所有页面。

您在这里看到的是Scrapy的跟进链接机制：当您在回调方法中产生请求时，Scrapy将安排该请求的发送并在该请求完成时注册要执行的回调方法。

使用此工具，您可以构建复杂的爬虫，并根据定义的规则跟进链接，并根据其访问的页面提取不同类型的数据。

在我们的示例中，它创建了一个循环，将其链接到下一页的所有链接，直到找不到该链接为止——便于爬取通过分页部署的网站比如博客、论坛。

#### 创建 Requests 的快捷方式

您可以使用 `response.follow` 作为创建 Request 对象的快捷方式：

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('span small::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)
```

与 `scrapy.Request` 不同的是， `response.follow` 支持相对链接——无需调用`urljoin`。请注意 `response.follow` 仅返回一个 Request 实例对象，你还需要 `yield` 这个实例。

您还可以将选择器传递给 `response.follow`。 该选择器应提取必要的属性：

```python
for href in response.css('ul.pager a::attr(href)'):
    yield response.follow(href, callback=self.parse)
```

对于 `<a>` 元素，有一个快捷方式：`response.follow` 自动使用其 `href` 属性。 因此，代码可以进一步缩短：

```python
for a in response.css('ul.pager a'):
    yield response.follow(a, callback=self.parse)
```

要从一个可迭代对象创建多个请求，可以改用 `response.follow_all`：

```python
anchors = response.css('ul.pager a')
yield from response.follow_all(anchors, callback=self.parse)
```

或者再简短些：

```python
yield from response.follow_all(css='ul.pager a', callback=self.parse)
```

#### 更多示例和模式

这是另一个回调和跟进后续链接的爬虫的例子，这次是用于抓取作者信息：

```python
import scrapy


class AuthorSpider(scrapy.Spider):
    name = 'author'

    start_urls = ['http://quotes.toscrape.com/']

    def parse(self, response):
        author_page_links = response.css('.author + a')
        yield from response.follow_all(author_page_links, self.parse_author)

        pagination_links = response.css('li.next a')
        yield from response.follow_all(pagination_links, self.parse)

    def parse_author(self, response):
        def extract_with_css(query):
            return response.css(query).get(default='').strip()

        yield {
            'name': extract_with_css('h3.author-title::text'),
            'birthdate': extract_with_css('.author-born-date::text'),
            'bio': extract_with_css('.author-description::text'),
        }
```

这个爬虫从首页开始，它将跟进指向作者页面的所有链接，并为每个页面调用 `parse_author` 回调方法。以及如前所见的带有 `parse` 回调的递归分页。

在这里，我们将回调传递给 `response.follow_all` 作为位置参数，以使代码更短； 它也适用于 `Request`。

`parse_author` 回调定义了一个辅助函数，用于从CSS查询中提取和清除数据，并生成包含作者数据的Python字典。

该爬虫演示的另一件有趣的事情是，即使同一位作者的名言很多，我们也不必担心会多次访问同一作者页面。 默认情况下，Scrapy 过滤掉对已访问URL的重复请求，从而避免了由于编程错误而导致服务器访问过多的问题。 可以通过设置 `DUPEFILTER_CLASS` 进行配置。

希望到目前为止，您已经对如何在 Scrapy 中使用跟进链接和回调的机制有了很好的了解。

作为另一个利用跟进链接机制的爬虫示例，请查看 `CrawlSpider` 类中的通用爬虫，该类实现了一个小的规则引擎，您可以在该规则上编写爬虫。

同样，一种常见的模式是使用一个[技巧将更多数据传递给回调]()，从而使用来自多个页面的数据来构建项目。

### 使用爬虫参数

您可以在运行爬虫时使用 `-a` 选项来为爬虫提供命令行参数：

```python
scrapy crawl quotes -o quotes-humor.json -a tag=humor
```

这些参数会传递给 Spider 的 `__init__` 方法，并默认成为爬虫属性

在此示例中，为 `tag` 参数提供的值可通过 `self.tag` 获得。 您可以使用它使您的Spider只获取带有特定 tag 的名言，并根据传入参数构建URL：

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        url = 'http://quotes.toscrape.com/'
        tag = getattr(self, 'tag', None)
        if tag is not None:
            url = url + 'tag/' + tag
        yield scrapy.Request(url, self.parse)

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```

如果您将参数 `tag=humor` 传给这个爬虫，您会发现它只访问 `humor` 标签下的 URL，比如 `http://quotes.toscrape.com/tag/humor`

您可以[在此处了解有关处理蜘蛛参数的更多信息](https://docs.scrapy.org/en/latest/topics/spiders.html#spiderargs).

### 下一步该干啥

本教程仅介绍了Scrapy的基础知识，但此处未提及许多其他功能。 [看看还有啥]()？ 在[初窥Scrapy]()中快速概述了最重要的概念。

您可以从“[基本概念]()”部分继续，以了解有关命令行工具，爬虫，选择器以及本教程未涵盖的其他内容（例如，对抓取的数据进行建模）的更多信息。 如果您喜欢看看示例项目，请查看“示例”部分。

## 示例

最好的学习方法是通过示例来学习，Scrapy也不例外。 因此，有一个名为 [quotesbot](https://github.com/scrapy/quotesbot) 的 Scrapy 项目，您可以使用它来玩和了解有关 Scrapy 的更多信息。 它包含两个用于[http://quotes.toscrape.com](http://quotes.toscrape.com) 的爬虫，一个使用CSS选择器，另一个使用XPath表达式。

[quotesbot](https://github.com/scrapy/quotesbot) 项目可从以下网址获得：https://github.com/scrapy/quotesbot。 您可以在项目的README 文件中找

如果您熟悉git，可以 checkout/clone 代码。不然的话您也可以通过单击 [此处](https://github.com/scrapy/quotesbot/archive/master.zip) 将项目下载为zip文件。