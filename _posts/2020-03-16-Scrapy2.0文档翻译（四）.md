---
layout: post
author: Berry Qiu
title: Scrapy2.0文档翻译（四）—基本概念（2）
subtitle: 基本概念 —— 爬虫 Spiders
cover: 
color: '' # 默认根据字数生成相应颜色
tags: ['Python', '文档']
category: public
---

# 基本概念

## 爬虫 Spiders

爬虫是定义如何抓取某个站点（或一组站点）的类，包括如何爬取（即跟进链接）以及如何从其页面中提取结构化数据（即抓取项 item）。 换句话说，爬虫 Spider是您自定义爬取和解析特定站点（或一组站点）页面的地方。

爬虫的抓取周期经历如下过程：

1. 生成初始 Requests 以爬取第一个URL，并指定要对下载下来的 Response 使用的回调函数。

   要执行的第一个请求是通过调用 `start_requests()` 方法获得的，该方法（默认情况下）生成对 `start_urls` 中指定的URL的 `Request`，并使用 `parse` 方法作为Requests的回调函数。

2. 在回调函数中，您解析 response（网页）并返回一个字典，该字典包括提取的数据，`Item` 对象，`Request` 对象或这些对象的可迭代对象。 这些 Requests 还将包含一个回调函数（可能相同），接着由Scrapy下载，并由指定的回调函数处理 response。

3. 在回调函数中，通常使用选择器 [Selectors]()（也可以使用BeautifulSoup，lxml或您喜欢的任何机制）来解析页面内容，并使用已解析的数据生成 items。

4. 最后，通常将 spider 返回的 items 保存到数据库（在某些[项目管道(Item Pipeline)]()中）或使用[Feed exports]()写入文件。

即使该周期（或多或少）适用于所有种类的爬虫，Scrapy中还是捆绑了不同类型的默认爬虫以用于不同目的。 我们将在此讨论这些类型。

### scrapy.Spider

#### **class scrapy.spiders.Spider** [[source]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders.html#Spider)

这是最简单的爬虫，也是所有其他爬虫都必须继承的（包括与 Scrapy 捆绑的爬虫，以及您自己编写的爬虫）。 它不提供任何特殊功能。 它只是提供了默认的 `start_requests()` 实现，该方法根据 spider的属性 `start_urls` 发送请求，并为每个返回的 response 调用爬虫的 `parse` 方法。

##### `name`

一个字符串，它定义此爬虫的名称。 爬虫名称是Scrapy查找（并实例化）爬虫的依据，因此它必须是唯一的。 但是，您也可以实例化同一爬虫的多个实例。 该属性是最重要的属性，且是**必需**的。

如果爬虫只抓取一个域名，通常的做法是使用该域命名爬虫，无论有无[TLD](https://en.wikipedia.org/wiki/Top-level_domain)。 例如，爬`mywebsite.com` 的爬虫被命名为 `mywebsite`。

##### `allowed_domains`

一个可选的列表，存储的是允许爬虫爬取的域名。如果 `OffsiteMiddleware` 开启的话，不属于该列表的域名（或子域名）的 URL 请求不会被跟进。

如果你的目标 url 是 `https://www.example.com/1.html`，那么就把 `'example.com'` 加进该列表中。

##### `start_urls`

未指定特定URL时，爬虫程序将从该 URL 列表开始爬取。 因此，下载的第一页将是此处列出的页面。 随后的 `Request` 将根据起始URL中包含的数据连续生成。

##### `custom_settings`

字典形式的设置项。在运行该 spider 时会覆盖项目配置。由于必须在实例化之前更新设置，因此必须将其定义为类属性。

有关可用的内置设置，请参阅：[内置设置参考手册]()。

##### `crawler`

在类初始化后，该属性通过类方法 `from_crawler()` 设置，并链接到该 spider 实例绑定到的 `Crawler` 对象中。

爬虫程序在项目中封装了许多组件以进行单项访问（例如扩展，中间件 middlewares，信号管理器 signals managers等）。 请参阅[Crawler API]()进一步了解它们。

##### `settings`

运行该 spider 的配置。这是一个 `Settings` 实例，详情请参阅[Settings]()主题。

##### `logger`

使用 Spider 的名称 `name` 创建的Python日志。 您可以通过它发送日志消息，如[从Spiders记录日志]()中所述。

##### `from_crawler(crawler, *args, **kwargs)`[[source\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders.html#Spider.from_crawler)

这是创建你的 spider 的类方法。

您可能不需要直接重写此方法，因为该方法默认实现充当`__init __()`方法的代理，并使用给定参数 `args` 和命名参数 `kwargs` 对其进行调用。

不过，此方法会在新实例中设置 `crawler` 和 `settings` 属性，以便以后可以在 spider 的代码内对其进行访问。

参数：

- **crawler** (`Crawler` 的实例) - 该 spider 绑定的 crawler
- **args** （列表list） - 传给 `__init__()` 方法的参数。
- **kwargs** （字典 dict）- 传给 `__init__()` 的关键字参数。

##### `start_requests()` [[源码\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders.html#Spider.start_requests)

该方法必须返回一个可迭代对象 (iterable)。该对象包含了 spider 用于爬取的第一个 Request。当 spider 准备爬取时， Scrapy 会调用它。Scrapy 只会调用一次该方法，因此可以将 `start_requests()` 实现为生成器（generator）。

默认的实现为：对 `start_url` 中的每个 url 生成 `Request(url, dont_filter=True)` 。

如果要更改用于开始抓取域的 Requests，则要重写该方法。 例如，如果您需要通过使用 POST 请求登录来开始，则可以执行以下操作：

```python
class MySpider(scrapy.Spider):
    name = 'myspider'

    def start_requests(self):
        return [scrapy.FormRequest("http://www.example.com/login",
                                   formdata={'user': 'john', 'pass': 'secret'},
                                   callback=self.logged_in)]

    def logged_in(self, response):
        # 在这里，您将提取链接以跟踪并返回每个链接的请求Request
        # Request 中使用另一个回调函数
        pass
```

##### `parse(response)` [[源码\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders.html#Spider.parse)

这是当请求 request 未指定回调回调时，Scrapy用于处理已下载响应的默认回调函数。

`parse` 方法负责处理响应，并返回抓取的数据和/或要跟进的URL。 其他Requests回调函数与`Spider` 类具有相同的要求。

此方法以及其他任何 Request 回调都必须返回 `Request` 和/或字典 `dict` 或 `Item` 对象的可迭代对象。

参数：

- **response**（`Response`）- 用于解析的响应。

##### `log(message[, level, component])` [[源码\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders.html#Spider.log)

一个通过 spider 的 [`logger`](#logger) 发送日志的封装，并保持向后兼容。更多信息请参见[从Spiders记录日志]()。

##### `closed(reason)`

spider 关闭时调用。此方法为 `spider_closed` 提供了前往 signal.connect() 的捷径。

---

我们先看一个例子：

```python
import scrapy


class MySpider(scrapy.Spider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = [
        'http://www.example.com/1.html',
        'http://www.example.com/2.html',
        'http://www.example.com/3.html',
    ]

    def parse(self, response):
        self.logger.info('A response from %s just arrived!', response.url)
```

从一个回调函数中返回多个 Requests 和 Items：

```python
import scrapy

class MySpider(scrapy.Spider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = [
        'http://www.example.com/1.html',
        'http://www.example.com/2.html',
        'http://www.example.com/3.html',
    ]

    def parse(self, response):
        for h3 in response.xpath('//h3').getall():
            yield {"title": h3}

        for href in response.xpath('//a/@href').getall():
            yield scrapy.Request(response.urljoin(href), self.parse)
```

您可以直接使用 `start_requests()` 代替 `start_urls`；为了使数据更具结构性，您可以使用[Items]()：

```python
import scrapy
from myproject.items import MyItem

class MySpider(scrapy.Spider):
    name = 'example.com'
    allowed_domains = ['example.com']

    def start_requests(self):
        yield scrapy.Request('http://www.example.com/1.html', self.parse)
        yield scrapy.Request('http://www.example.com/2.html', self.parse)
        yield scrapy.Request('http://www.example.com/3.html', self.parse)

    def parse(self, response):
        for h3 in response.xpath('//h3').getall():
            yield MyItem(title=h3)

        for href in response.xpath('//a/@href').getall():
            yield scrapy.Request(response.urljoin(href), self.parse)
```

### Spider 参数

spider 可以接收修改其行为的参数。 spider 参数的一些常见用法是定义起始URL或限制爬取网站的某些部分。不过其实它们可用于配置 spider 的任何功能。

spider 参数通过 `crawl` 命令的 `-a` 选项传递，比如：

```bash
scrapy crawl myspider -a category=electronics
```

spider 可以在其 `__init__` 方法中访问参数：

```python
import scrapy

class MySpider(scrapy.Spider):
    name = 'myspider'

    def __init__(self, category=None, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = ['http://www.example.com/categories/%s' % category]
        # ...
```

默认的 `__init__` 方法能接受任何 spider 参数，并将其作为 spider 的属性。 上面的示例也可以编写如下：

```python
import scrapy

class MySpider(scrapy.Spider):
    name = 'myspider'

    def start_requests(self):
        yield scrapy.Request('http://www.example.com/categories/%s' % self.category)
```

请记住：spider 参数只会是字符串。spider 不会对其采用任何解析方法。如果您想通过命令行设置 `start_urls`，你需要自己将它解析成列表（比如用 ast.literal_eval 或 json.loads），然后再设成属性。不然的话，将会在 `start_urls` 字符串上进行迭代（非常常见的python陷阱），从而导致每个字符都被视为单独的 url。

一个有效的用例是使用 `HttpAuthMiddleware` 设置 http 身份验证或使用 `UserAgentMiddleware` 设置用户代理（user agent）：

```bash
scrapy crawl myspider -a http_user=myuser -a http_pass=mypassword -a user_agent=mybot
```

spider 参数也可以通过 Scrapyd `schedule.json`API传递。 请参阅[Scrapyd文档](https://scrapyd.readthedocs.io/en/latest/)。

### 通用爬虫 Spider

Scrapy 附带了一些有用的通用爬虫，您可以让您的 spider 继承它们。 他们的目的是为一些常见的爬取情况提供便利的功能，例如根据某些规则在网站上跟踪所有链接、从[Sitemap](https://www.sitemaps.org/index.html)进行爬取或解析XML/CSV feed。

在下面的 spider 案例中，我们假设您有一个 Scrapy 项目，并在 `myproject.items` 模块中声明了 `TestItem`类，如下：

```python
import scrapy

class TestItem(scrapy.Item):
    id = scrapy.Field()
    name = scrapy.Field()
    description = scrapy.Field()
```

#### CrawlingSpider

`class scrapy.spiders.CrawlSpider` [[源码\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/crawl.html#CrawlSpider)

这是用于爬取常规网站的最常用的 spider，因为它通过定义一组规则为跟踪链接提供了便捷的机制。 它可能不是最适合您特定网站或项目的方法，但是它在很多情况下都通用，因此您可以从中入手，并根据需要覆盖它以实现更多自定义功能，或者只是实现自己的Spider。

除了从 Spider 继承的属性（必须指定）之外，此类还支持一个新属性：

- `rules` ：一个（或多个）`Rule` 对象的列表。 每个 `Rule` 都定义了爬取网站的特定行为。 Rules 对象将在下面介绍。 如果多个规则与同一链接匹配，则将按照在此属性中定义的顺序使用第一个规则。

该 spider 还公开了一个可重写的方法：

- `parse_start_url(response)` [[源码\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/crawl.html#CrawlSpider.parse_start_url) ：该方法用于 `start_urls` 响应。 它允许解析初始响应，并且必须返回 `Item` 对象、`Request` 对象或二者中任何一个的可迭代对象。

##### Crawling 规则

`class scrapy.spiders.Rule(link_extractor=None, callback=None, cb_kwargs=None,  follow=None, process_links=None, process_request=None, errback=None)`

- `link_extractor`是一个链接提取器（Link Extracotr）对象，它定义如何从每个已爬取页面提取链接。 每个产生的链接将用于生成一个 `Request` 对象，该对象将在其 `meta` 词典中（在`link_text`键下）包含链接文本。 如果省略，将使用不带参数创建的默认链接提取器，从而提取所有链接。

- `callback` 是为使用指定的 `link_extractor` 提取的每个链接调用的 callable 或 string（如果是字符串，该 spider 中同名的函数将被调用）。该回调将 `Response` 作为其第一个参数，并且必须返回单个实例或 `Item`，`dict` 和/或 `Request` 对象（或其任何子类）的可迭代对象。 如上所述，接收到的 `Response` 对象包含了 `Request` 的 `meta` 字典中（在`link_text`键下）的链接文本。

  警告：在编写爬虫规则时，请避免使用 `parse` 作为回调函数。因为 `CrawlSpider` 使用自己的 `parse` 方法实现它的逻辑，如果您重写了这个 `parse` 方法，该 crawl spider 将不再工作。

- `cb_kwargs` 是一个字典。它包含了传入回调函数的关键字参数。

- `follow` 是布尔值。它指定是否应从使用此规则提取的每个响应中跟进链接。 如果 `callback` 为 None，则 `follow` 默认为 `True`，否则默认为 `False`。

- `process_links` 可以是 callable 或 string（如果是字符串，该 spider 中同名的函数将被调用）。 从 `link_extractor` 中获取到链接列表时将会调用该函数。该方法主要用来过滤。

- `process_request` 是一个 callable 或 string（如果是字符串，该 spider 中同名的函数将被调用）。 该规则提取到每个 `Request` 时都会调用该函数。该方法需要将 request 作为第一个参数，而每个 request 对应的 `Response` 作为第二个参数。该函数必须返回一个 `Request` 或者  `None`。 (用来过滤 request) 

- `errback` 是一个 callable 或 string（如果是字符串，该 spider 中同名的函数将被调用）。当处理该规则的 request 时引发了异常，该方法即被调用。它接收 `Twisted Failure` 作为第一个参数。

  *2.0版本的新特性： errback 参数*

##### CrawlSpider 示例

下面是配合 rule 使用 CrawlSpider 的例子：

```python
import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

class MySpider(CrawlSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com']

    rules = (
        # 提取匹配了 'category.php' 的链接 (不匹配 'subsection.php')
        # 并跟进它们的链接 (因为没有指定 callback，故 follow 默认为 True).
        Rule(LinkExtractor(allow=('category\.php', ), deny=('subsection\.php', ))),

        # 提取匹配了 'item.php' 的链接，并使用 parse_item 解析。
        Rule(LinkExtractor(allow=('item\.php', )), callback='parse_item'),
    )

    def parse_item(self, response):
        self.logger.info('Hi, this is an item page! %s', response.url)
        item = scrapy.Item()
        item['id'] = response.xpath('//td[@id="item_id"]/text()').re(r'ID: (\d+)')
        item['name'] = response.xpath('//td[@id="item_name"]/text()').get()
        item['description'] = response.xpath('//td[@id="item_description"]/text()').get()
        item['link_text'] = response.meta['link_text']
        return item
```

该 spider 将从 example.com 的首页开始爬取，获取 category 以及 item 的链接并对后者使用 `parse_item` 方法。 对每个 item 的响应，将使用 XPath 处理 HTML 并生成一些数据填入 [`Item`]() 中。

#### XMLFeedSpider

`class scrapy.spiders.XMLFeedSpider` [[源码\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/feed.html#XMLFeedSpider)

XMLFeedSpider用于通过按特定节点名称对其进行迭代来解析XML feed。可以从 `iternodes`，`xml` 和 `html` 中选择迭代器。 出于性能方面的考虑，建议使用 `iternodes` 迭代器，因为 `xml` 和 `html` 迭代器会读取整个DOM进行解析。 不过，在解析标记错误的 `XML` 时，使用 `html` 作为迭代器可能会很有用。

要设置迭代器和标签名称 (tag name)，必须定义以下类属性：

- `iterator`

  string 类型，定义要使用的迭代器，可以是：

  - `'iternodes'` - 基于正则表达式的高性能迭代器
  - `'html'` - 使用 [`Selector`]() 的迭代器。需要注意的是该迭代器使用 DOM 进行分析，其需要将所有的 DOM 载入内存， 当数据量大的时候会产生问题。
  - `'xml'` - 使用 [`Selector`]() 的迭代器。需要注意的是该迭代器使用 DOM 进行分析，其需要将所有的 DOM 载入内存， 当数据量大的时候会产生问题。

  默认值为 `'iternodes'`。

- `itertag`：string 类型，其中包含要迭代的节点（或元素）的名称。比如：

  ```python
  itertag = 'product'
  ```

- `namespaces`

  一个由 `(prefix, url)` 元组 (tuple) 所组成的 list。 其定义了在该文档中会被 spider 处理的可用的 namespace。 `prefix` 及 `uri` 会被自动调用 [`register_namespace()`]() 生成 namespace。

  您可以通过在 `itertag` 属性中制定节点的 namespace。

  示例：

  ```python
  class YourSpider(XMLFeedSpider):
  
      namespaces = [('n', 'http://www.sitemaps.org/schemas/sitemap/0.9')]
      itertag = 'n:url'
      # ...
  ```


除了这些新属性之外，该 spider 有以下可重写的方法：

- `adapt_response(response)` [[源码\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/feed.html#XMLFeedSpider.adapt_response)

  一种在 spider 开始解析之前，从 spider 中间件（middleware）接收响应的方法。您可以在 response 被分析之前使用该函数来修改内容 (body)。 该方法接受一个 response 并返回一个 response (可以相同也可以不同)。

- `parse_node(response, selector)` [源码\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/feed.html#XMLFeedSpider.parse_node)

  当节点符合提供的标签名时 (`itertag`) 该方法被调用。 接收到的 response 以及相应的 [`Selector`]() 作为参数传递给该方法。 该方法返回一个 [`Item`]() 对象或者 [`Request`]() 对象 或者一个包含二者的可迭代对象 (iterable)。

- `process_results(response, results)` [[source\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/feed.html#XMLFeedSpider.process_results)

  当 spider 返回结果 (item 或 request) 时该方法被调用。 设定该方法的目的是在结果返回给框架核心 (framework core) 之前做最后的处理， 例如设定 item 的 ID。其接受一个结果的列表 (list of results) 及对应的 response。 其结果必须返回一个结果的列表 (list of results)(包含 Item 或者 Request 对象)。

##### XMLFeedSpider 示例

该爬虫十分易用，让我们看看下面这个例子：

```python
from scrapy.spiders import XMLFeedSpider
from myproject.items import TestItem

class MySpider(XMLFeedSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com/feed.xml']
    iterator = 'iternodes'  # This is actually unnecessary, since it's the default value
    itertag = 'item'

    def parse_node(self, response, node):
        self.logger.info('Hi, this is a <%s> node!: %s', self.itertag, ''.join(node.getall()))

        item = TestItem()
        item['id'] = node.xpath('@id').get()
        item['name'] = node.xpath('name').get()
        item['description'] = node.xpath('description').get()
        return item
```

简单来说，我们创建了一个 spider，从给定的 `start_urls` 中下载 feed， 并迭代 feed 中每个 `item` 标签，输出，并往 [`Item`]() 中存储一些随机数据。

#### CSVFeedSpider

`class scrapy.spiders.CSVFeedSpider` [[源码\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/feed.html#CSVFeedSpider)

该 spider 除了其按行遍历而不是节点之外其他和 XMLFeedSpider 十分类似。 而其在每次迭代时调用的是 `parse_row()` 。

- `delimiter`

  在 CSV 文件中用于区分字段的分隔符。类型为 string。 默认为 `','` (逗号)。

- `quotechar`

  在 CSV 文件中字段的附加字符。类型为 string。默认为  `'"'` (引号)。

- `headers`

  类型为 list。存放 CSV 文件中的列名。

- `parse_row(response, row)` [[源码\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/feed.html#CSVFeedSpider.parse_row)

  该方法接收一个 response 对象及一个以提供或检测出来的 header 为键的字典 (代表每行)。 该 spider 中，您也可以覆盖 `adapt_response` 及 `process_results` 方法来进行预处理 (pre-processing) 及后 (post-processing) 处理。

##### CSVFeedSpider 示例

下面的例子和之前的例子很像，但使用了 `CSVFeedSpider`:

```python
from scrapy.spiders import CSVFeedSpider
from myproject.items import TestItem

class MySpider(CSVFeedSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com/feed.csv']
    delimiter = ';'
    quotechar = "'"
    headers = ['id', 'name', 'description']

    def parse_row(self, response, row):
        self.logger.info('Hi, this is a row!: %r', row)

        item = TestItem()
        item['id'] = row['id']
        item['name'] = row['name']
        item['description'] = row['description']
        return item
```

#### SitemapSpider

`class scrapy.spiders.SitemapSpider` [[源码\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/sitemap.html#SitemapSpider)

#### SitemapSpider

##### SitemapSpider example

SitemapSpider 允许您通过使用 Sitemaps 发现URL来爬取网站。

其支持嵌套的 sitemap，并能从 [robots.txt](http://www.robotstxt.org/) 中获取 sitemap 的 url。

- `sitemap_urls`

  包含您要爬取的 url 的 sitemap 的 url 列表 (list)。 您也可以指定为一个 [robots.txt](http://www.robotstxt.org/) ，spider 会从中分析并提取 url。

- `sitemap_rules`

  一个包含 `(regex, callback)` 元组的列表 (list):

  - `regex` 是一个用于匹配从 sitemap 提供的 url 的正则表达式。 `regex` 可以是一个字符串或者编译的正则对象 (compiled regex object)。
  - callback 指定了匹配正则表达式的 url 的处理函数。 `callback` 可以是一个字符串 (spider 中方法的名字) 或者是 callable。

  示例：

  ```python
  sitemap_rules = [('/product/', 'parse_product')]
  ```

  规则按顺序进行匹配，之后第一个匹配才会被应用。

  如果您忽略该属性，sitemap 中发现的所有 url 将会被 `parse` 函数处理。

- `sitemap_follow`

  一个用于匹配要跟进的 sitemap 的正则表达式的列表 (list)。其仅仅被应用在使用 [Sitemap index files](https://www.sitemaps.org/protocol.html#index) 来指向其他 sitemap 文件的站点。

  默认情况下所有的 sitemap 都会被跟进。

- `sitemap_alternate_links`

  指定当一个 `url` 有可选的链接时，是否跟进。 有些非英文网站会在一个 `url` 块内提供其他语言的网站链接。

  示例：

  ```xml
  <url>
      <loc>http://example.com/</loc>
      <xhtml:link rel="alternate" hreflang="de" href="http://example.com/de"/>
  </url>
  ```

  当 `sitemap_alternate_links` 设置时，两个 URL 都会被获取。 当 `sitemap_alternate_links` 关闭时，只有 `http://example.com/` 会被获取。

  默认 `sitemap_alternate_links` 关闭。

- `sitemap_filter(entries)` [[源码\]](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/sitemap.html#SitemapSpider.sitemap_filter)

  这是一个筛选函数，可重写该函数，以根据其属性选择 sitemap entries。

  示例：

  ```xml
  <url>
      <loc>http://example.com/</loc>
      <lastmod>2005-01-01</lastmod>
  </url>
  ```

  我们可以写一个 `sitemap_filter` 函数根据日期筛选 `entries`：

  ```python
  from datetime import datetime
  from scrapy.spiders import SitemapSpider
  
  class FilteredSitemapSpider(SitemapSpider):
      name = 'filtered_sitemap_spider'
      allowed_domains = ['example.com']
      sitemap_urls = ['http://example.com/sitemap.xml']
  
      def sitemap_filter(self, entries):
          for entry in entries:
              date_time = datetime.strptime(entry['lastmod'], '%Y-%m-%d')
              if date_time.year >= 2005:
                  yield entry
  ```

  这将仅检索 2005 年及以后修改过的 `entries`。

  entries 是从 sitemap document 中提取的字典（dict）对象。通常使用标签名（tag name）作为键（key），使用标签内的文本（text）作为值（value）。

  非常重要：

  - 因为需要loc属性，所以将删除没有此标签的 entries
  - 备用链接（alternate links）存储成链表，并作为键 `alternate` 的值保存。（参见 `sitemap_alternate_links`）
  - 命名空间（namespace）被移除了。所以 lxml 标签 `{namespace}tagname` 变成仅有 `tagname`。

  如果您忽略该方法，sitemap 中所有的 entries 都将被处理。

##### SitemapSpider 示例

最简单的例子：使用 `parse` 方法处理 sitemap 中发现的所有 url：

```python
from scrapy.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = ['http://www.example.com/sitemap.xml']

    def parse(self, response):
        pass # ... 在这里抓取 item ...
```

用特定的回调函数处理某些 url，其他的使用另外的回调函数:

```python
from scrapy.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = ['http://www.example.com/sitemap.xml']
    sitemap_rules = [
        ('/product/', 'parse_product'),
        ('/category/', 'parse_category'),
    ]

    def parse_product(self, response):
        pass # ... 抓取 product ...

    def parse_category(self, response):
        pass # ... 抓取 category ...
```

跟进 [robots.txt](http://www.robotstxt.org/) 文件定义的 sitemap 并只跟进包含有 `/sitemap_shop` 的 url:

```python
from scrapy.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = ['http://www.example.com/robots.txt']
    sitemap_rules = [
        ('/shop/', 'parse_shop'),
    ]
    sitemap_follow = ['/sitemap_shops']

    def parse_shop(self, response):
        pass # ... 在这里抓取 shop ...
```

在 SitemapSpider 中使用其他 url:

```python
from scrapy.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = ['http://www.example.com/robots.txt']
    sitemap_rules = [
        ('/shop/', 'parse_shop'),
    ]

    other_urls = ['http://www.example.com/about']

    def start_requests(self):
        requests = list(super(MySpider, self).start_requests())
        requests += [scrapy.Request(x, self.parse_other) for x in self.other_urls]
        return requests

    def parse_shop(self, response):
        pass # ... 在这里抓取 shop ...

    def parse_other(self, response):
        pass # ... 在这里抓取 other ...
```

