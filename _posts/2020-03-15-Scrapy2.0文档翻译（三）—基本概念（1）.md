---
layout: post
author: Berry Qiu
title: Scrapy2.0文档翻译（三）—基本概念（1）
subtitle: 基本概念——命令行工具
cover: 
color: '' # 默认根据字数生成相应颜色
tags: ['Python', '文档']
category: public
---

# 基本概念

## 命令行工具

*0.10 版本中的新内容*

Scrapy 是通过 `scrapy` 命令行工具控制的，在这里称为“ Scrapy工具”，以将其与子命令区分开。我们将其子命令称为“命令”或“ Scrapy命令”。

Scrapy工具提供了多个命令，用于多种用途，每个命令接受一组不同的参数和选项。

（为了支持独立的 `scrapyd-deploy`，在 1.0 中已删除 `scrapy deploy` 命令。请参阅[部署您的项目](https://scrapyd.readthedocs.io/en/latest/deploy.html)。）

### 配置设置

Scrapy将在以下目录的 `scrapy.cfg` 文件中查找配置参数：

1. `/etc/scrapy.cfg` 或 `c:\scrapy\scrapy.cfg` （系统范围），
2. `~/.config/scrapy.cfg` (`$XDG_CONFIG_HOME`) 和 `~/.scrapy.cfg` (`$HOME`) 作为全局（用户范围）设置，
3. Scrapy 项目根目录中的 `scrapy.cfg`（请参阅下一部分）。

这些文件中的设置将按照列出的优先顺序进行合并：用户定义的值的优先级高于系统范围的默认值。如果项目定义了设置的话，项目范围的设置将覆盖所有其他设置。

Scrapy 还能通过许多环境变量进行配置。 目前有：

- `SCRAPY_SETTINGS_MODULE` (请参阅 [指定设置]())
- `SCRAPY_PROJECT` (请参阅 [在项目之间共享根目录]())
- `SCRAPY_PYTHON_SHELL` (请参阅 [Scrapy shell]())

### Scrapy 项目的默认结构

在研究命令行工具及其子命令之前，首先让我们了解 Scrapy 项目的目录结构。

尽管可以修改，但默认情况下，所有Scrapy项目都具有相同的文件结构，类似下面这样：

```bash
scrapy.cfg
myproject/
    __init__.py
    items.py
    middlewares.py
    pipelines.py
    settings.py
    spiders/
        __init__.py
        spider1.py
        spider2.py
        ...
```

`scrapy.cfg` 文件所在的目录称为项目根目录。 `scrapy.cfg` 包含定义项目设置的python模块的名称。 举个栗子：

```properties
[settings]
default = myproject.settings
```

### 在项目之间共享根目录

一个项目根目录（包含 `scrapy.cfg` 的目录）可以由多个 Scrapy 项目共享，每个项目都有自己的设置模块。

在这种情况下，必须在 `scrapy.cfg` 文件的 `[settings]` 下为这些设置模块定义一个或多个别名：

```properties
[settings]
default = myproject1.settings
project1 = myproject1.settings
project2 = myproject2.settings
```

默认情况下，`scrapy` 命令行工具使用默认 `default` 设置。 使用 `SCRAPY_PROJECT` 环境变量来指定其他项目以供 `scrapy` 使用：

```bash
$ scrapy settings --get BOT_NAME
Project 1 Bot
$ export SCRAPY_PROJECT=project2
$ scrapy settings --get BOT_NAME
Project 2 Bot
```

### 使用 `scrapy` 工具

您可以在命令行运行不带参数的 Scrapy 工具，它将打印使用帮助以及可用命令。

```bash
Scrapy X.Y - no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  crawl         Run a spider
  fetch         Fetch a URL using the Scrapy downloader
[...]
```

如果您在一个 Scrapy 项目中，第一行将打印当前的活动项目。在上面这个例子中是在项目外运行`scrapy`，如果在项目内部运行的话，会打印类似下面的信息：

```bash
Scrapy X.Y - project: myproject

Usage:
  scrapy <command> [options] [args]

[...]
```

#### 创建项目

通常来说，使用 `scrapy` 工具要做的第一件事是创建 Scrapy 项目：

```powershell
scrapy startproject myproject [project_dir]
```

这将在 `project_dir` 目录下创建一个 Scrapy 项目。 如果未指定 `project_dir` ，则 `project_dir`将与 `myproject` 相同。

接下来，进入到新项目的目录内：

```powershell
cd project_dir
```

您已经准备好使用 `scrapy` 命令管理和控制您的项目。

#### 控制项目

您可以从项目内部使用 `scrapy` 工具来控制和管理它们。

比如，新建一个爬虫：

```bash
scrapy genspider mydomain mydomain.com
```

某些 Scrapy 命令（比如 `crawl`）必须在 Scrapy 项目内部才能运行。请参阅下面的[参考命令]()，以查看哪些命令必须在项目内部运行，而哪些不是必须在项目内部运行。

还请记住，从项目内部运行某些命令时，它们的行为可能略有不同。 例如，如果要获取的 url 与某些特定的 Spider 关联，则 `fetch` 命令将覆盖爬虫（例如 `user_agent` 属性覆盖user-agent）。 这是有意的，因为 `fetch` 命令是用于检查 Spider 如何下载页面的。

### 可用的命令

本节包含可用的内置命令的列表，并带有说明和一些用法示例。 请记住，您始终可以通过运行以下命令获取有关每个命令的更多信息：

```bash
scrapy <command> -h
```

您可以使用以下命令查看所有可用命令：

```bash
scrapy -h
```

有两种命令，一种只能在 Scrapy 项目内部运行（项目特定命令）。另一种也可以在没有活动的Scrapy 项目内部运行（全局命令），尽管它们在项目内部运行时的行为可能略有不同（ 因为他们会使用项目的覆盖设置）。

全局命令：

- [`startproject`](#startproject)
- [`genspider`](#genspider)
- [`settings`](settings)
- [`runspider`](#runspider)
- [`shell`](#shell)
- [`fetch`](#fetch)
- [`view`](#view)
- [`version`](#version)

项目特定命令：

- [`crawl`](#crawl)
- [`check`](#check)
- [`list`](#list)
- [`edit`](#edit)
- [`parse`](#parse)
- [`bench`](#bench)

#### startproject

- 语法：`scrapy startproject <project_name> [project_dir]`
- 是否需要项目：否

在 `project_dir` 目录下创建一个新的 Scrapy 项目，项目名称为 `project_name`，如果没有指定 `project_dir`，`project_dir` 与 `project_name` 一致。

使用示例：

```bash
$ scrapy startproject myproject
```

#### genspider

- 语法：`scrapy genspider [-t template] <name> <domain>`
- 是否需要项目：否

如果在一个 Scrapy 项目下运行的话，会在当前项目的 `spiders` 文件夹下新建一个爬虫，如果不在一个项目下运行，会在当前文件夹下新建一个爬虫。`<name>` 参数指定了蜘蛛的名字 `name`，`<domain>` 用于生成爬虫的 `allowed_domains` 及 `start_urls` 属性。

使用示例：

```bash
$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider example example.com
Created spider 'example' using template 'basic'

$ scrapy genspider -t crawl scrapyorg scrapy.org
Created spider 'scrapyorg' using template 'crawl'
```

这只是用于基于预定义模板创建爬虫的快捷命令，但肯定不是唯一的创建爬虫的方法。 您可以自己创建爬虫源代码文件，而不使用此命令。

#### crawl

- 语法：`scrapy crawl <spider>`
- 是否需要项目：是

指定一个爬虫开始爬取。

使用示例：

```bash
$ scrapy crawl myspider
[ ... myspider starts crawling ... ]
```

#### check

- 语法：`scrapy check [-l] <spider>`
- 是否需要项目：是

运行 contract 检查

使用示例：

```bash
$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
>>> 'RetailPricex' field is missing

[FAILED] first_spider:parse
>>> Returned 92 requests, expected 0..4
```

#### list

- 语法：`scrapy list`
- 是否需要项目：是

列出当前项目中所有可用的爬虫。每行输出一个爬虫。

使用示例：

```bash
$ scrapy list
spider1
spider2
```

#### edit

- 语法：`scrapy edit <spider>`
- 是否需要项目：是

使用 [`EDITOR`]() 中设定的编辑器编辑给定的爬虫。

该命令仅仅是提供一个快捷方式。开发者可以自由选择其他工具或者 IDE 来编写调试爬虫。

使用示例：

```bash
$ scrapy edit spider1
```

#### fetch

- 语法：`scrapy fetch <url>`
- 是否需要项目：否

使用 Scrapy 下载器 (downloader) 下载给定的 URL，并将获取到的内容送到标准输出。

关于此命令的有趣之处在于，该命令以爬虫 spider 下载页面的方式获取页面。例如，如果 spider 有 `USER_AGENT` 属性修改了 User Agent，该命令将会使用该属性。

因此，您可以使用该命令来查看爬虫如何获取某个特定页面。

如果在项目外部运行该命令，则使用默认的 Scrapy 下载器（downloader）设定。

支持的选项：

- `--spider=SPIDER`：跳过爬虫自动检测并强制使用特定的爬虫
- `--headers`：打印响应的 HTTP 头，而不是响应的主体
- `--no-redirect`：不遵循 HTTP 3xx重定向（默认为遵循它们）

使用示例：

```bash
$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{'Accept-Ranges': ['bytes'],
 'Age': ['1263   '],
 'Connection': ['close     '],
 'Content-Length': ['596'],
 'Content-Type': ['text/html; charset=UTF-8'],
 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],
 'Etag': ['"573c1-254-48c9c87349680"'],
 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],
 'Server': ['Apache/2.2.3 (CentOS)']}
```

#### view

- 语法：`scrapy view <url>`
- 是否需要项目：否

在浏览器中打开给定的 URL ，该 URL 为 Scrapy 爬虫所“看到”的。 有时，爬虫看到的页面与普通用户的页面不同，因此可以用来检查爬虫“看到”的内容并确认它是您想要的。

支持的选项：

- `--spider=SPIDER`：跳过爬虫自动检测并强制使用特定的爬虫
- `--no-redirect`：不遵循 HTTP 3xx重定向（默认为遵循它们）

使用示例：

```bash
$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
```

#### shell

- 语法：`scrapy shell [url]`
- 是否需要项目：否

以给定的URL（也可以为空）启动 Scrapy shell。支持 UNIX 风格的本地文件路径，包括相对路径前缀 `./`、`../`，和绝对路径。更多内容请参见 [Scrapy shell]()。

支持的选项：

- `--spider=SPIDER`：跳过爬虫自动检测并强制使用特定的爬虫

- `-c code`：打印代码 `code` 的结果并退出

- `--no-redirect`：不遵循 HTTP 3xx重定向（默认为遵循它们）

  您可以在命令行中将 URL 作为参数传递； 一旦进入 shell，默认情况下`fetch(url)` 仍将遵循HTTP重定向。

使用示例：

```bash
$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]

$ scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'
(200, 'http://www.example.com/')

# shell follows HTTP redirects by default
$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
(200, 'http://example.com/')

# you can disable this with --no-redirect
# (only for the URL passed as command line argument)
$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
(302, 'http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F')
```

#### parse

- 语法：`scrapy parse <url> [options]`
- 是否需要项目：是

获取给定的 URL 并使用相应的爬虫解析。如果有 `--callback` 选项，使用给定方法进行解析，若没有，使用 `parse` 解析。

支持的选项：

- `--spider=SPIDER`：跳过爬虫自动检测并强制使用特定的爬虫
- `--a NAME=VALUE`：设置爬虫的参数 (可能被重复)
- `--callback` 或 `-c`：爬虫中用于解析 response 的回调函数
- `--meta` 或 `-m`：传给 request 回调函数的额外 request meta。必须是一个有效的 JSON 字符串。例： `-meta='{"foo":"bar"}'`
- `--cbkwargs`：传给回调函数的额外关键字。必须是一个有效的 JSON 字符串。例：`-cbkwargs='{"foo":"bar"}'`
- `--pipelines`：在 pipeline 中处理 item
- `--rules` 或 `-r`：使用 [`CrawlSpider`]() 规则来发现用来解析 response 的回调函数
- `--noitems`：不显示爬取到的 item
- `--links`：不显示提取到的链接
- `--nocolour`：避免使用 pygments 对输出着色
- `--depth` 或 `-d`：指定递归跟进链接请求的层次数 (默认: 1)
- `--verbose` 或 `-v`: 显示每个深度级别的详细信息

使用示例：

```bash
$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

>>> STATUS DEPTH LEVEL 1 <<<
# Scraped Items  ------------------------------------------------------------
[{'name': 'Example item',
 'category': 'Furniture',
 'length': '12 cm'}]

# Requests  -----------------------------------------------------------------
[]
```

#### settings

- 语法：`scrapy setting [options]`
- 是否需要项目：否

获取 Scrapy 的设定

如果在项目中运行，会显示项目的设置。如果不在项目中运行，会显示默认的 Scrapy 设置。

使用示例：

```bash
$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
```

#### runspider

- 语法：`scrapy runspider <spider_file.py>`
- 是否需要项目：否

在未创建项目的情况下，运行一个编写在 Python 文件中的爬虫。

使用示例：

```bash
$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]
```

#### version

- 语法：`scrapy version [-v]`
- 是否需要项目：否

打印 Scrapy 版本。配合 `-v` 运行时，该命令同时输出 Python, Twisted 以及平台的信息，方便 bug 提交。

#### bench

*0.17*新版本中的新内容

- 语法：`scrapy bench`
- 是否需要项目：否

运行 benchmark 测试。 [Benchmarking]() 。

### 自定义项目命令

您也可以通过 [`COMMANDS_MODULE`](#COMMANDS_MODULE) 来添加您自己的项目命令。您可以以 [scrapy/commands](https://github.com/scrapy/scrapy/tree/master/scrapy/commands) 中 Scrapy commands 为例来了解如何实现您的命令。

#### COMMANDS_MODULE

默认：`''` (空字符串)

一个查询自定义 Scrapy 命令的模块。用于向您的 Scrapy 项目中添加自定义命令。

示例：

```python
COMMANDS_MODULE = 'mybot.commands'
```

#### 通过setup.py入口点注册命令

注意：这是一项实验性功能，请谨慎使用。

您还可以通过在外部库的 `setup.py` 文件的入口点添加 `scrapy.commands` 来从外部库添加 Scrapy 命令。

以下示例为添加 `my_command` 命令：

```python
from setuptools import setup, find_packages

setup(name='scrapy-mymodule',
  entry_points={
    'scrapy.commands': [
      'my_command=my_scrapy_module.commands:MyCommand',
    ],
  },
 )
```

