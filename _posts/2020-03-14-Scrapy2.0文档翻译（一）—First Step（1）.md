---
layout: post
author: Berry Qiu
title: Scrapy2.0文档翻译（一）—First Step（1）
subtitle: 这是 First Step 的第一部分
cover: 
color: '' # 默认根据字数生成相应颜色
tags: ['Python', '文档']
category: public
---

## 初窥Scrapy

Scrapy是一个用于爬网网站并提取结构化数据的应用程序框架，可用于各种有用的应用程序，例如数据挖掘，信息处理或历史档案。

即使Scrapy最初是为了Web抓取而设计的，它也可以用于调用API（例如Amazon Associates Web Services）或作为通用网络爬虫来提取数据。

### 爬虫示例

为了向您展示 Scrapy 带来的好处，我们将使用最简单的爬虫的方法向您介绍Scrapy Spider。

以下是从网站 [http://quotes.toscrape.com](http://quotes.toscrape.com/) 的分页下抓取名人名言的爬虫代码：

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = 'quotes'
    start_urls = [
        'http://quotes.toscrape.com/tag/humor/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'author': quote.xpath('span/small/text()').get(),
                'text': quote.css('span.text::text').get(),
            }

        next_page = response.css('li.next a::attr("href")').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```

将这段代码放进一个文本文件中，重命名为 `quotes_spider.py` ，然后通过 [`runspider`](https://docs.scrapy.org/en/latest/topics/commands.html#std:command-runspider) 命令运行这个爬虫：

```powershell
scrapy runspider quotes_spider.py -o quotes.json
```

完成此操作后，您将在 `quotes.json` 文件看到以 JSON 格式保存的名人名言，其中包含文本和作者，如下所示（此处重新格式化以提高可读性）：

```json
[{
    "author": "Jane Austen",
    "text": "\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d"
},
{
    "author": "Groucho Marx",
    "text": "\u201cOutside of a dog, a book is man's best friend. Inside of a dog it's too dark to read.\u201d"
},
{
    "author": "Steve Martin",
    "text": "\u201cA day without sunshine is like, you know, night.\u201d"
},
...]
```

#### 刚刚发生了什么？

当您运行命令 `scrapy runspider quotes_spider.py` 时，Scrapy 会在其中查找 Spider 定义，并通过爬虫引擎运行它。

此次的爬取一开始向 `start_urls` 属性中定义的URL（本示例中为 `humor` 标签下的名人名言的URL）发出请求，并调用默认的回调方法 `parse`，将响应对象作为参数传递。 在回调函数 `parse` 中，我们使用CSS选择器循环引用元素，生成包含提取的名人名言的文本和作者的Python字典，查找指向下一页的链接，并使用相同的 `parse` 方法作为回调函数进行另一个请求。

在这里，您会注意到 Scrapy 的主要优势之一：请求是异步调度和处理的。 这意味着 Scrapy 无需等待请求完成和处理，它可以同时发送另一个请求或执行其他操作。 这也意味着即使某些请求失败或在处理过程中发生错误，其他请求也可以继续执行。

虽然这可以让您能够非常快速地爬取（同时发送多个并发请求且能容错），但是 Scrapy 还使您可以通过一些设置来控制您爬取的礼貌程度。 比如您可以设置每个请求之间的下载延迟，限制每个域或每个IP的并发请求数量，甚至使用能够自动计算爬取速度的 auto-throttle 扩展。

**注意**：本示例使用提要导出 (feed exports) 来生成JSON文件，您可以轻松更改导出格式（例如XML或CSV）或存储后端（例如FTP或Amazon S3）。 您还可以编写项目管道 (item pipeline) 以将项目存储在数据库中。

### 还有什么好活？

您已经了解了如何使用 Scrapy 从网站提取和存储项目，但这仅仅是冰山一角。 Scrapy 提供了许多强大的功能，使抓取变得简单而有效，例如：

- 内置支持使用扩展的CSS选择器和XPath表达式从 HTML/XML 源中[选择和提取数据]()，以及使用正则表达式提取的辅助方法。
- [交互式Shell控制台]()（支持 IPython），用于测试CSS和XPath表达式的数据挖掘，在编写或调试 Spider 时非常有用。
- 内置支持以多种格式（JSON，CSV，XML）[生成提要导出]()并将其存储在多个后端（FTP，S3，本地文件系统）中
- 强大的编码支持和自动检测功能，用于处理外来的，非标准的和损坏的编码声明。
- [强大的可扩展性支持]()，使您可以使用信号和定义明确的API（中间件，扩展和管道）插入自己的功能。
- 广泛的内置扩展和中间件，用于处理：
  - Cookie 和 Session
  - HTTP功能，例如压缩，身份验证，缓存
  - 用户代理 (user-agent) 欺骗
  - robots.txt
  - 爬取深度限制
  - 更多等你来找哦
- [Telnet控制台]()，用于挂接到Scrapy进程中运行的Python控制台中，以内省和调试您的搜寻器
- 还有其他好东西比如可重复使用蜘蛛从Sitemaps和XML/CSV的提要进行爬取、用于[自动下载图片]()（或任何其他媒体）的媒体管道、可缓存的DNS解析器等等！

### 接下来是啥？

接下来的步骤是安装Scrapy，按照教程学习如何创建一个成熟的 Scrapy 项目并[加入社区](https://scrapy.org/community/)。

## 安装指南

### 安装 Scrapy

Scrapy 在Python 3.5或更高版本上运行。

如果您使用的是 [Anaconda](https://docs.anaconda.com/anaconda/) 或 [Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html)，则可以从 [conda-forge](https://conda-forge.org/) 渠道安装该软件包，该渠道包含适用于Linux，Windows和macOS的最新软件包。

要使用 `conda` 安装 Scrapy，运行：

```powershell
conda install -c conda-forge scarpy
```

另外，如果您已经熟悉Python软件包的安装，则可以使用以下方法从PyPI安装Scrapy及其依赖项：

```powershell
pip install Scrapy
```

请注意，有时这可能需要解决某些Scrapy依赖项的编译问题，具体取决于您的操作系统，因此请务必查看[特定于平台的安装说明]()。

强烈建议您在[专用的virtualenv]()中安装Scrapy，以避免与系统软件包冲突。

有关更多详细信息和特定于平台的说明以及故障排除信息，请继续阅读。

#### 最好看一看

Scrapy是用纯Python编写的，它依赖于几个关键的Python包(以及其他包)

- [lxml](https://lxml.de/index.html)，一个高效的 XML / HTML 解析器
- [parsel](https://pypi.org/project/parsel/)，基于 lxml 之上编写的 HTML / XML 数据提取库，
- [w3lib](https://pypi.org/project/w3lib/)，用于处理URL和网页编码的多功能辅助器
- [twisted](https://twistedmatrix.com/trac/)，异步网络框架
- [cryptography](https://cryptography.io/en/latest/) 和 [pyOpenSSL](https://pypi.org/project/pyOpenSSL/)，处理各种网络级安全需求

Scrapy测试的最低版本为：

- Twisted 14.0
- lxml 3.4
- pyOpenSSL 0.14

Scrapy可以使用这些软件包的较早版本，但不能保证其正常工作，因为尚未对其进行测试。

其中一些软件包本身依赖于非Python软件包，根据您的平台，这些软件包可能需要其他安装步骤。 请查看以下[特定平台的指南](#特定平台安装说明)。

如果存在与这些依赖项有关的问题，请参考它们各自的安装说明：

- [lxml 安装说明](https://lxml.de/installation.html)
- [cryptography 安装说明](https://cryptography.io/en/latest/installation/)

#### 使用虚拟环境（推荐）

我们建议在所有平台上的虚拟环境中安装Scrapy。

Python软件包既可以全局安装（也称为系统范围），也可以安装在用户空间中。 我们不建议在整个系统上安装Scrapy。

相反，我们建议您在所谓的“虚拟环境”（[`venv`](https://docs.python.org/3/library/venv.html#module-venv)）中安装Scrapy。 虚拟环境允许您与已经安装的Python系统软件包不冲突（这可能会破坏您的某些系统工具和脚本），并且仍然可以使用 `pip` 正常安装软件包（没有`sudo`等）。

有关如何创建虚拟环境的信息，请参见[虚拟环境和软件包](https://docs.python.org/3/tutorial/venv.html#tut-venv)。

创建虚拟环境后，就可以像其他任何Python软件包一样，通过pip在其内部安装Scrapy。 （有关您可能需要事先安装的非Python依赖项，请参见下面的[特定平台的指南](#特定平台安装说明)）。

### 特定平台安装说明

#### Windows

尽管可以使用 pip 在 Windows 上安装 Scrapy ，但是我们建议您安装 Anaconda 或 Miniconda 并使用 conda-forge 渠道中的软件包，这样可以避免大多数安装问题。

安装 Anaconda 或 Miniconda 后，请使用以下方法安装 crapy：

```powershell
conda install -c conda-forge scrapy
```

#### Ubuntu 14.04 及以上版本

目前，Scrapy已使用最新版本的 lxml，twisted和 pyOpenSSL进行了测试，并且与最新的 Ubuntu发行版兼容。 但是它也应该支持Ubuntu的较早版本，例如Ubuntu 14.04，尽管存在TLS连接的潜在问题。

**不要**使用Ubuntu提供的 `python-scrapy` 软件包，它们通常太旧且太慢，无法赶上最新的Scrapy。

要在Ubuntu（或基于Ubuntu的）系统上安装Scrapy，您需要安装以下依赖项：

```bash
sudo apt-get install python3 python3-dev python3-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev
```

- `lxml` 需要 `python3-dev`, `zlib1g-dev`, `libxml2-dev` 和 `libxslt1-dev`  
- `cryptography` 需要 `libssl-dev` 和 `libffi-dev` 

在virtualenv内，可以使用 `pip` 安装 Scrapy：

```
pip install scrapy
```

注意：可以使用相同的非Python依赖项在Debian Jessie（8.0）及更高版本中安装Scrapy。

#### macOS

构建 Scrapy 的依赖项需要使用C编译器和开发头文件。 在macOS上，这通常由 Apple 的 Xcode 开发工具提供。 要安装 Xcode 命令行工具，请打开一个终端窗口并运行：

```bash
xcode-select --install
```

有一个[已知的问题](https://github.com/pypa/pip/issues/2468)，导致 `pip` 无法更新系统软件包。 必须解决此问题才能成功安装 Scrapy 及其依赖项。 以下是一些建议的解决方案：

- （推荐）不要使用系统python，请安装与系统其余部分不冲突的新更新版本。 以下是使用 homebrew 程序包管理器的方法：

  - 根据 https://brew.sh/ 的说明安装 homebrew

  - 更新您的 `PATH` 变量，声明应在系统软件包之前使用 homebrew 软件包（如果将 `zsh` 用作默认 shell ，则将 `.bashrc` 更改为 `.zshrc`）：

    ```bash
    echo "export PATH=/usr/local/bin:/usr/local/sbin:$PATH" >> ~/.bashrc
    ```

  - 重新载入 `.bashrc` 以确保发生了更改。

    ```bash
    source ~/.bashrc
    ```

  - 安装 python

    ```bash
    brew install python
    ```

  - 最新版本的 python 捆绑了 `pip` ，因此您无需单独安装。 如果不是这种情况，请升级python：

    ```bash
    brew update; brew upgrade python
    ```

- （可选）[在Python虚拟环境内部安装 Scrapy](#使用虚拟环境（推荐）)

  此方法是解决上述macOS问题的一种解决方法，但它是管理依赖项的总体良好做法，且可以对第一种方法进行补充。

这些解决方法中的任何一种之后，您都应该能够安装Scrapy：

```bash
pip install Scrapy
```

### PyPy

我们建议使用最新的PyPy版本。 测试的版本是5.9.0。 对于PyPy3，仅测试了Linux安装。

现在，大多数Scrapy依赖项都具有用于CPython的二进制轮子，但没有用于PyPy的二进制轮子。 这意味着将在安装过程中建立这些依赖关系。 在macOS上，您可能会遇到构建 cryptography 依赖项的问题，[此处](https://github.com/pyca/cryptography/issues/2692#issuecomment-272773481)描述了此问题的解决方案，即 `brew install openssl`，然后导出此命令建议的标志（仅在安装Scrapy时需要）。 在Linux上安装除了安装构建依赖项外没有其他特殊问题。 未在Windows上测试使用PyPy安装Scrapy。

您可以通过运行 `scrapy bench` 检查 Scrapy 是否正确安装。如果该命令给出错误，比如 `TypeError: ... got 2 unexpected keyword arguments`，这意味着 setuptools 无法获得一个特定的 PyPy 依赖。可以运行 `pip install 'PyPyDispatcher>=2.1.0'` 以修复该问题。

### 疑难解答

#### AttributeError: ‘module’ object has no attribute ‘OP_NO_TLSv1_1’

在安装或升级Scrapy、Twisted或pyOpenSSL之后，您可能会收到带有以下回溯的异常：

```
[…]
  File "[…]/site-packages/twisted/protocols/tls.py", line 63, in <module>
    from twisted.internet._sslverify import _setAcceptableProtocols
  File "[…]/site-packages/twisted/internet/_sslverify.py", line 38, in <module>
    TLSVersion.TLSv1_1: SSL.OP_NO_TLSv1_1,
AttributeError: 'module' object has no attribute 'OP_NO_TLSv1_1'
```

出现此异常的原因是您的系统或虚拟环境具有 pyOpenSSL 版本，而 Twisted 版本不支持该版本。

要安装您的Twisted版本支持的 pyOpenSSL 版本，请使用 `tls` 选项重新安装Twisted：

```bash
pip install twisted[tls]
```

更多细节请查看 [Issue #2473](https://github.com/scrapy/scrapy/issues/2473)